{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learning SQL with Google BigQuery.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqipqz30vLVN45P8dFJFJT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishek-Dasani/learnings/blob/main/Learning_SQL_with_Google_BigQuery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Kaggle Tutorial on SQL\n",
        "\n",
        "[BigQuery docs](https://cloud.google.com/bigquery/docs)"
      ],
      "metadata": {
        "id": "puz1y9VRjKtO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJDbMca81tPs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a \"Client\" object\n",
        "client = bigquery.Client()\n",
        "\n",
        "# Construct a reference to the \"hacker_news\" dataset\n",
        "dataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n",
        "\n",
        "# API request - fetch the dataset\n",
        "dataset = client.get_dataset(dataset_ref)"
      ],
      "metadata": {
        "id": "v3_VAd52DyeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all the tables in the \"hacker_news\" dataset\n",
        "tables = list(client.list_tables(dataset))\n",
        "\n",
        "# Print number of tables in the dataset\n",
        "print(len(tables))\n",
        "\n",
        "# Print information on all the columns in the \"full\" table in the \"hacker_news\" dataset\n",
        "table.schema\n",
        "\n",
        "# Print names of all tables in the dataset (there are four!)\n",
        "for table in tables:  \n",
        "    print(table.table_id)"
      ],
      "metadata": {
        "id": "x4Fc5NpVmJ3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a reference to the \"full\" table\n",
        "table_ref = dataset_ref.table(\"full\")\n",
        "\n",
        "# API request - fetch the table\n",
        "table = client.get_table(table_ref)\n",
        "\n",
        "# Preview the first five lines of the \"full\" table\n",
        "client.list_rows(table, max_results=5).to_dataframe()\n",
        "\n",
        "# Preview the first five entries in the \"by\" column of the \"full\" table\n",
        "client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()"
      ],
      "metadata": {
        "id": "GXnp0c0AmWKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "examples of query using different datasets"
      ],
      "metadata": {
        "id": "3TkfOnOJnIgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a \"Client\" object\n",
        "client = bigquery.Client()\n",
        "\n",
        "# Construct a reference to the \"openaq\" dataset\n",
        "dataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\n",
        "\n",
        "# API request - fetch the dataset\n",
        "dataset = client.get_dataset(dataset_ref)\n",
        "\n",
        "# List all the tables in the \"openaq\" dataset\n",
        "tables = list(client.list_tables(dataset))\n",
        "\n",
        "# Print names of all tables in the dataset (there's only one!)\n",
        "for table in tables:  \n",
        "    print(table.table_id)"
      ],
      "metadata": {
        "id": "w3-yWAoMmfoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a reference to the \"global_air_quality\" table\n",
        "table_ref = dataset_ref.table(\"global_air_quality\")\n",
        "\n",
        "# API request - fetch the table\n",
        "table = client.get_table(table_ref)\n",
        "\n",
        "# Preview the first five lines of the \"global_air_quality\" table\n",
        "client.list_rows(table, max_results=5).to_dataframe()"
      ],
      "metadata": {
        "id": "DCUZk3fvnQkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to select all the items from the \"city\" column where the \"country\" column is 'US'\n",
        "query = \"\"\"\n",
        "        SELECT city\n",
        "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
        "        WHERE country = 'US'\n",
        "        \"\"\"\n",
        "\n",
        "# Note that when writing an SQL query, the argument we pass to FROM is not in single or double quotation marks (' or \"). It is in backticks (`)."
      ],
      "metadata": {
        "id": "hThVKU25nRyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the query\n",
        "query_job = client.query(query)\n",
        "\n",
        "# API request - run the query, and return a pandas DataFrame\n",
        "us_cities = query_job.to_dataframe()\n",
        "\n",
        "# What five cities have the most measurements?\n",
        "us_cities.city.value_counts().head()"
      ],
      "metadata": {
        "id": "WT-Wj4cancNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BigQuery datasets can be huge. We allow you to do a lot of computation for free, but everyone has some limit.\n",
        "\n",
        "Each Kaggle user can scan 5TB every 30 days for free. Once you hit that limit, you'll have to wait for it to reset.\n",
        "\n",
        "The biggest dataset currently on Kaggle is 3TB, so you can go through your 30-day limit in a couple queries if you aren't careful.\n",
        "\n",
        "Don't worry though: we'll teach you how to avoid scanning too much data at once, so that you don't run over your limit.\n",
        "\n",
        "To begin,you can estimate the size of any query before running it. Here is an example using the (very large!) Hacker News dataset. To see how much data a query will scan, we create a QueryJobConfig object and set the dry_run parameter to True"
      ],
      "metadata": {
        "id": "wwLVezRqoSux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to get the score column from every row where the type column has value \"job\"\n",
        "query = \"\"\"\n",
        "        SELECT score, title\n",
        "        FROM `bigquery-public-data.hacker_news.full`\n",
        "        WHERE type = \"job\" \n",
        "        \"\"\"\n",
        "\n",
        "# Create a QueryJobConfig object to estimate size of query without running it\n",
        "dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n",
        "\n",
        "# API request - dry run query to estimate costs\n",
        "dry_run_query_job = client.query(query, job_config=dry_run_config)\n",
        "\n",
        "print(\"This query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed))"
      ],
      "metadata": {
        "id": "Trwt-hfdoTqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also specify a parameter when running the query to limit how much data you are willing to scan. Here's an example with a low limit."
      ],
      "metadata": {
        "id": "HzIwKM8_o1s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only run the query if it's less than 1 MB\n",
        "ONE_MB = 1000*1000\n",
        "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)\n",
        "\n",
        "# Set up the query (will only run if it's less than 1 MB)\n",
        "safe_query_job = client.query(query, job_config=safe_config)\n",
        "\n",
        "# API request - try to run the query, and return a pandas DataFrame\n",
        "safe_query_job.to_dataframe()"
      ],
      "metadata": {
        "id": "MCvCcJpco0YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to select comments that received more than 10 replies\n",
        "query_popular = \"\"\"\n",
        "                SELECT parent, COUNT(id)\n",
        "                FROM `bigquery-public-data.hacker_news.comments`\n",
        "                GROUP BY parent\n",
        "                HAVING COUNT(id) > 10\n",
        "                \"\"\""
      ],
      "metadata": {
        "id": "mmVqlABJrx9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the query (cancel the query if it would use too much of \n",
        "# your quota, with the limit set to 10 GB)\n",
        "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
        "query_job = client.query(query_popular, job_config=safe_config)\n",
        "\n",
        "# API request - run the query, and convert the results to a pandas DataFrame\n",
        "popular_comments = query_job.to_dataframe()\n",
        "\n",
        "# Print the first five rows of the DataFrame\n",
        "popular_comments.head()"
      ],
      "metadata": {
        "id": "NAFX4BTQ4_Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aliasing and other improvements\n",
        "\n",
        "A couple hints to make your queries even better:\n",
        "\n",
        "* The column resulting from COUNT(id) was called f0__. That's not a very descriptive name. You can change the name by adding AS NumPosts after you specify the aggregation. This is called aliasing, and it will be covered in more detail in an upcoming lesson.\n",
        "* If you are ever unsure what to put inside the COUNT() function, you can do COUNT(1) to count the rows in each group. Most people find it especially readable, because we know it's not focusing on other columns. It also scans less data than if supplied column names (making it faster and using less of your data access quota).\n",
        "\n",
        "Using these tricks, we can rewrite our query:"
      ],
      "metadata": {
        "id": "57V0aMYd5C84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved version of earlier query, now with aliasing & improved readability\n",
        "query_improved = \"\"\"\n",
        "                 SELECT parent, COUNT(1) AS NumPosts\n",
        "                 FROM `bigquery-public-data.hacker_news.comments`\n",
        "                 GROUP BY parent\n",
        "                 HAVING COUNT(1) > 10\n",
        "                 \"\"\"\n",
        "\n",
        "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
        "query_job = client.query(query_improved, job_config=safe_config)\n",
        "\n",
        "# API request - run the query, and convert the results to a pandas DataFrame\n",
        "improved_df = query_job.to_dataframe()\n",
        "\n",
        "# Print the first five rows of the DataFrame\n",
        "improved_df.head()"
      ],
      "metadata": {
        "id": "JsrYU14k5IW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note on using GROUP BY\n",
        "\n",
        "Note that because it tells SQL how to apply aggregate functions (like COUNT()), it doesn't make sense to use GROUP BY without an aggregate function. Similarly, if you have any GROUP BY clause, then all variables must be passed to either a\n",
        "\n",
        "* GROUP BY command, or\n",
        "* an aggregation function.\n",
        "\n",
        "Note that there are two variables: parent and id.\n",
        "\n",
        "* parent was passed to a GROUP BY command (in GROUP BY parent), and\n",
        "* id was passed to an aggregate function (in COUNT(id)).\n"
      ],
      "metadata": {
        "id": "XOjZQ4KV5SJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Create a \"Client\" object\n",
        "client = bigquery.Client()\n",
        "\n",
        "# Construct a reference to the \"nhtsa_traffic_fatalities\" dataset\n",
        "dataset_ref = client.dataset(\"nhtsa_traffic_fatalities\", project=\"bigquery-public-data\")\n",
        "\n",
        "# API request - fetch the dataset\n",
        "dataset = client.get_dataset(dataset_ref)\n",
        "\n",
        "# Construct a reference to the \"accident_2015\" table\n",
        "table_ref = dataset_ref.table(\"accident_2015\")\n",
        "\n",
        "# API request - fetch the table\n",
        "table = client.get_table(table_ref)\n",
        "\n",
        "# Preview the first five lines of the \"accident_2015\" table\n",
        "client.list_rows(table, max_results=5).to_dataframe()"
      ],
      "metadata": {
        "id": "qhdPNQHv5eo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to find out the number of accidents for each day of the week\n",
        "query = \"\"\"\n",
        "        SELECT COUNT(consecutive_number) AS num_accidents, \n",
        "               EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week\n",
        "        FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n",
        "        GROUP BY day_of_week\n",
        "        ORDER BY num_accidents DESC\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "J-Zyo__kmeNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the query (cancel the query if it would use too much of \n",
        "# your quota, with the limit set to 1 GB)\n",
        "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**9)\n",
        "query_job = client.query(query, job_config=safe_config)\n",
        "\n",
        "# API request - run the query, and convert the results to a pandas DataFrame\n",
        "accidents_by_day = query_job.to_dataframe()\n",
        "\n",
        "# Print the DataFrame\n",
        "accidents_by_day"
      ],
      "metadata": {
        "id": "TzdJkc47mjEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Create a \"Client\" object\n",
        "client = bigquery.Client()\n",
        "\n",
        "# Construct a reference to the \"world_bank_intl_education\" dataset\n",
        "dataset_ref = client.dataset(\"world_bank_intl_education\", project=\"bigquery-public-data\")\n",
        "\n",
        "# API request - fetch the dataset\n",
        "dataset = client.get_dataset(dataset_ref)\n",
        "\n",
        "# Construct a reference to the \"international_education\" table\n",
        "table_ref = dataset_ref.table(\"international_education\")\n",
        "\n",
        "# API request - fetch the table\n",
        "table = client.get_table(table_ref)\n",
        "\n",
        "# Preview the first five lines of the \"international_education\" table\n",
        "client.list_rows(table, max_results=5).to_dataframe()"
      ],
      "metadata": {
        "id": "zPl7Gwiamls0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "country_spend_pct_query = \"\"\"\n",
        "                          SELECT country_name, AVG(value) AS avg_ed_spending_pct\n",
        "                          FROM `bigquery-public-data.world_bank_intl_education.international_education`\n",
        "                          WHERE indicator_code = \"SE.XPD.TOTL.GD.ZS\" AND year >= 2010 AND year <= 2017 \n",
        "                          GROUP BY country_name\n",
        "                          ORDER BY avg_ed_spending_pct DESC\n",
        "                          \"\"\"\n",
        "\n",
        "# Set up the query (cancel the query if it would use too much of \n",
        "# your quota, with the limit set to 1 GB)\n",
        "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
        "country_spend_pct_query_job = client.query(country_spend_pct_query, job_config=safe_config)\n",
        "\n",
        "# API request - run the query, and return a pandas DataFrame\n",
        "country_spending_results = country_spend_pct_query_job.to_dataframe()\n",
        "\n",
        "# View top few rows of results\n",
        "print(country_spending_results.head())"
      ],
      "metadata": {
        "id": "SoUCfBNcopnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here\n",
        "code_count_query = \"\"\"SELECT indicator_code, indicator_name, COUNT(1) AS num_rows\n",
        "                        FROM `bigquery-public-data.world_bank_intl_education.international_education`\n",
        "                        WHERE year = 2016\n",
        "                        GROUP BY indicator_code, indicator_name\n",
        "                        HAVING num_rows >= 175\n",
        "                        ORDER BY num_rows DESC\n",
        "                    \"\"\"\n",
        "\n",
        "# Set up the query\n",
        "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
        "code_count_query_job = client.query(code_count_query, job_config=safe_config)\n",
        "\n",
        "# API request - run the query, and return a pandas DataFrame\n",
        "code_count_results = code_count_query_job.to_dataframe()\n",
        "\n",
        "# View top few rows of results\n",
        "print(code_count_results.head())"
      ],
      "metadata": {
        "id": "KnuBFFrNqShN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WITH ... AS\n",
        "\n",
        "On its own, AS is a convenient way to clean up the data returned by your query. It's even more powerful when combined with WITH in what's called a \"common table expression\".\n",
        "\n",
        "A common table expression (or CTE) is a temporary table that you return within your query. CTEs are helpful for splitting your queries into readable chunks, and you can write queries against them.\n",
        "\n",
        "Also, it's important to note that CTEs only exist inside the query where you create them, and you can't reference them in later queries. So, any query that uses a CTE is always broken into two parts: (1) first, we create the CTE, and then (2) we write a query that uses the CTE."
      ],
      "metadata": {
        "id": "2Ab3w5TLuGpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Create a \"Client\" object\n",
        "client = bigquery.Client()\n",
        "\n",
        "# Construct a reference to the \"crypto_bitcoin\" dataset\n",
        "dataset_ref = client.dataset(\"crypto_bitcoin\", project=\"bigquery-public-data\")\n",
        "\n",
        "# API request - fetch the dataset\n",
        "dataset = client.get_dataset(dataset_ref)\n",
        "\n",
        "# Construct a reference to the \"transactions\" table\n",
        "table_ref = dataset_ref.table(\"transactions\")\n",
        "\n",
        "# API request - fetch the table\n",
        "table = client.get_table(table_ref)\n",
        "\n",
        "# Preview the first five lines of the \"transactions\" table\n",
        "client.list_rows(table, max_results=5).to_dataframe()"
      ],
      "metadata": {
        "id": "ntDME9sFuHWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the block_timestamp column contains the date of each transaction in DATETIME format, we'll convert these into DATE format using the DATE() command.\n",
        "\n",
        "We do that using a CTE, and then the next part of the query counts the number of transactions for each date and sorts the table so that earlier dates appear first."
      ],
      "metadata": {
        "id": "DPXwr1s_uQYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to select the number of transactions per date, sorted by date\n",
        "query_with_CTE = \"\"\" \n",
        "                 WITH time AS \n",
        "                 (\n",
        "                     SELECT DATE(block_timestamp) AS trans_date\n",
        "                     FROM `bigquery-public-data.crypto_bitcoin.transactions`\n",
        "                 )\n",
        "                 SELECT COUNT(1) AS transactions,\n",
        "                        trans_date\n",
        "                 FROM time\n",
        "                 GROUP BY trans_date\n",
        "                 ORDER BY trans_date\n",
        "                 \"\"\"\n",
        "\n",
        "# Set up the query (cancel the query if it would use too much of \n",
        "# your quota, with the limit set to 10 GB)\n",
        "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
        "query_job = client.query(query_with_CTE, job_config=safe_config)\n",
        "\n",
        "# API request - run the query, and convert the results to a pandas DataFrame\n",
        "transactions_by_date = query_job.to_dataframe()\n",
        "\n",
        "# Print the first five rows\n",
        "transactions_by_date.head()"
      ],
      "metadata": {
        "id": "Z-Fyd5OpuZ_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, common table expressions (CTEs) let you shift a lot of your data cleaning into SQL. That's an especially good thing in the case of BigQuery, because it is vastly faster than doing the work in Pandas."
      ],
      "metadata": {
        "id": "Kyu_eu39ueGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oezLSNliunRy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}